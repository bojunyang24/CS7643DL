%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Proofs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{enumerate}[resume]
\item (3 points) Prove that
%
\begin{align}
\log_e x\leq x-1, \qquad \forall x>0
\end{align}
%
with equality if and only if $x=1$.

[\emph{Hint:} Consider differentiation of $\log(x)-(x-1)$ and think about concavity/convexity and second derivatives.]
\pagebreak
\item (6 points)
Consider two discrete probability distributions $p$ and $q$ over $k$ outcomes:
%
\begin{subequations}
\begin{align}
\sum_{i=1}^k p_i = \sum_{i=1}^k q_i=1 \\
p_i > 0, q_i > 0, \quad \forall i \in \{1,\ldots,k\}
\end{align}
\end{subequations}
%
The Kullback-Leibler (KL) divergence (also known as the \emph{relative entropy}) between these distributions is given by:
%
\begin{equation}
KL(p,q)=\sum_{i=1}^{k} p_i\log\left(\frac{p_i}{q_i}\right)
\end{equation}
%
It is common to refer to $KL(p,q)$ as a measure of distance (even though it is not a proper metric).
Many algorithms in machine learning are based on minimizing KL divergence between two
probability distributions.
In this question, we will show why this might be a sensible thing to do.\\

[\emph{Hint:} This question doesn't require you to know anything more than the definition of $KL(p,q)$ and the identity
in Q$7$]

\begin{enumerate}
\item Using the results from Q$7$, show that $KL(p,q)$ is always non-negative.
\pagebreak
\item When is $KL(p,q) = 0$?
\pagebreak
\item Provide a counterexample to show that the KL divergence is not a symmetric function of its arguments: $KL(p,q) \neq KL(q,p)$
\end{enumerate}

\pagebreak
\item
(6 points) In this question, we will get familiar with a fairly popular and useful function, called the log-sum-exp function. For $\vec{x} \in \mathbb{R}^n$, the log-sum-exp function is defined (quite literally) as:
\beqn
f(\vec{x}) = \log\bigg(\sum_{i=1}^n e^{x_i}\bigg)
\eeqn
\begin{enumerate}
\item Prove that $f(\vec{x})$ is differentiable everywhere in $\mathbb{R}^n$.

[\emph{Hint:} Multivariable functions are differentiable if the partial derivatives exist and are continuous.]
\pagebreak
\item Prove that $f(\vec{x})$ is convex on $\mathbb{R}^n$.

[\emph{Hint:} One approach is to use the second-order condition for convexity.]
\pagebreak
\item Show that $f(\vec{x})$ can be viewed as an approximation of the max function, bounded as follows:
\beqn
\max\{x_1, \dots, x_n\} \le f(\vec{x}) \le \max\{x_1, \dots, x_n\} + \log(n)
\eeqn
\pagebreak
\end{enumerate}
\end{enumerate}
